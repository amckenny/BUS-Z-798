{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9c6414-cf9f-4cb5-afc1-c8bfc59937ef",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f48c47-da4a-48eb-bd06-558d565be357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import functools, os, re, shutil, string, sys\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk, pyLDAvis, spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scattertext as st\n",
    "import tensorflow as tf\n",
    "import tomotopy as tp\n",
    "import xgboost as xgb\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from scipy.stats import pointbiserialr\n",
    "from sklearn import linear_model, naive_bayes, svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dict_analysis import read_dictionary, get_count\n",
    "\n",
    "nltk_dir = Path.cwd()/\"models\"\n",
    "assert nltk_dir.exists()\n",
    "os.environ[\"NLTK_DATA\"] = str(nltk_dir)\n",
    "os.environ['OMP_NUM_THREADS'] = '16'\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10854f-eedf-4bfe-9bd1-54174730e7e7",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eec334-50bf-4780-ba47-4e2d61368e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tf.config.list_physical_devices(\"GPU\"):\n",
    "    print(\"No GPU detected... You probably want to restart with a GPU-enabled runtime\")\n",
    "else:\n",
    "    print(\"Found a GPU! Ready for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917a9bb-77d5-490f-8555-2cca2b38f354",
   "metadata": {},
   "source": [
    "If you see a message above stating that no GPU is detected, make sure you are running this from an interactive GPU session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0699434-4c55-4308-86a1-212a7e613e9a",
   "metadata": {},
   "source": [
    "## Loading the IMDB dataset\n",
    "\n",
    "For simplicity, we're going to use a large publicly available dataset to demonstrate machine learning using multiple techniques. This dataset has already been loaded for you in the setup process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a9f1c-788a-43e7-8479-4dd5c5b356df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts_path = Path.cwd() / \"texts\"\n",
    "dicts_path = Path.cwd() / \"dictionaries\"\n",
    "dataset_dir = texts_path / 'aclImdb'\n",
    "train_dir = dataset_dir / 'train'\n",
    "test_dir = dataset_dir / 'test'\n",
    "\n",
    "# Generate Samples for the NN\n",
    "batch_size = 32\n",
    "seed = 24601\n",
    "\n",
    "print(f\"\\n====Loading Training Dataset==== - {datetime.now()}\", flush=True)\n",
    "raw_train = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "class_names = raw_train.class_names\n",
    "train_ds = raw_train.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"\\n====Loading Validation Dataset==== - {datetime.now()}\", flush=True)\n",
    "raw_validation = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "val_ds = raw_validation.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"\\n====Loading Test Dataset==== - {datetime.now()}\", flush=True)\n",
    "raw_test = tf.keras.preprocessing.text_dataset_from_directory(test_dir, batch_size=batch_size)\n",
    "test_ds = raw_test.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad917d6a-bba1-4156-9282-77629ee70402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training sample to pandas for non-tensorflow processing (i.e., traditional ML)\n",
    "print(f\"\\n====Converting Training Sample to Pandas==== - {datetime.now()}\", flush=True)\n",
    "reviews = []\n",
    "sentiments = []\n",
    "for text, sentiment in train_ds.as_numpy_iterator():\n",
    "  reviews.extend(text)\n",
    "  sentiments.extend(sentiment)\n",
    "\n",
    "full_train_data = pd.DataFrame(zip(reviews,sentiments), columns=['review','sentiment'])\n",
    "\n",
    "# Convert training sample to pandas for non-tensorflow processing (i.e., traditional ML)\n",
    "print(f\"\\n====Converting Validation Sample to Pandas==== - {datetime.now()}\", flush=True)\n",
    "reviews = []\n",
    "sentiments = []\n",
    "for text, sentiment in val_ds.as_numpy_iterator():\n",
    "  reviews.extend(text)\n",
    "  sentiments.extend(sentiment)\n",
    "\n",
    "full_validation_data = pd.DataFrame(zip(reviews,sentiments), columns=['review','sentiment'])\n",
    "\n",
    "# Convert test sample to pandas for non-tensorflow processing (e.g., traditional ML, dictionary-based coding)\n",
    "print(f\"\\n====Converting Test Sample to Pandas==== - {datetime.now()}\", flush=True)\n",
    "reviews = []\n",
    "sentiments = []\n",
    "for text, sentiment in test_ds.as_numpy_iterator():\n",
    "  reviews.extend(text)\n",
    "  sentiments.extend(sentiment)\n",
    "\n",
    "full_test_data = pd.DataFrame(zip(reviews,sentiments), columns=['review','sentiment'])\n",
    "\n",
    "print(f\"\\n====Generating 3000 Row Test Dataset Subsample (for demonstration time-saving purposes)==== - {datetime.now()}\", flush=True)\n",
    "np.random.seed(seed)\n",
    "test_data = full_test_data.sample(n=3000)\n",
    "test_data['txt_sent'] = test_data.apply(lambda x: \"Positive\" if x[\"sentiment\"]==1 else \"Negative\", axis=1)\n",
    "test_data['review'] = test_data['review'].apply(lambda x: x.decode(\"utf-8\").replace(\"\\\\\", \"\").replace(\"<br />\", \" \"))\n",
    "\n",
    "# Stopwords setup\n",
    "print(f\"\\n====Getting Stopwords Setup==== - {datetime.now()}\", flush=True)\n",
    "stops = nltk.corpus.stopwords.words('english')+[\"'s\", '&']\n",
    "\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae32f8-faa5-426a-b416-53f3c76ddf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionaries\n",
    "dictionaries = {}\n",
    "for file in dicts_path.glob(f'*.dict'):\n",
    "  dictionary_data = read_dictionary(file)\n",
    "  dictionaries[dictionary_data['var_name']] = dictionary_data\n",
    "\n",
    "#Spin up preprocessing pipeline\n",
    "print(\"Preprocessing texts - this may take a while\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "docs = nlp.pipe(test_data['review'].tolist())\n",
    "del nlp\n",
    "\n",
    "# Update the pandas dataframe with the tokenized reviews\n",
    "preprocessed = []\n",
    "for doc in docs:\n",
    "    preprocessed.append(\n",
    "        [token.text.lower()\n",
    "         for token in doc\n",
    "         if token.pos_ not in [\"PUNCT\", \"SYM\", \"NUM\", 'X']\n",
    "         and token.text.lower() not in stops\n",
    "        ]\n",
    "    )\n",
    "test_data['review_tokens'] = preprocessed\n",
    "\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527dc2ae-9e15-4853-9cb6-b37e235c4923",
   "metadata": {},
   "source": [
    "# Rules-based analyses as a point for comparison\n",
    "---\n",
    "\n",
    "This data set links IMDB movie reviews to the overall 'sentiment' (whether it's a positive review or negative review) of the movie. Let's take a look at a few of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57189a76-efa8-477a-81d1-aa2c206f7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch from the training set and print first five reviews\n",
    "for _, row  in test_data.head().iterrows():\n",
    "    print(f\"Review: {row['review'][:130]}...\")\n",
    "    print(f\"Sentiment: {row['txt_sent']}\\n\")\n",
    "\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea141b-d497-4ec0-9397-33e934103137",
   "metadata": {},
   "source": [
    "In dictionary-based coding, we look at the frequency with which a list of words thought to be associated with a construct (in this case, positive/negative sentiment) appear in a corpus of texts.\n",
    "\n",
    "The selection of words in the dictionaries is *crucial* for measure validity and reliability. In this case, we're looking at positive/negative words from Henry (2008). Let's take a look at these dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5e0e1-282f-43e6-a605-4f685d4e0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out all dictionaries' names, titles, and word lists.\n",
    "for name, dictionary in dictionaries.items():\n",
    "    print(f\"\\nNAME: {name}\")\n",
    "    print(f\"TITLE: {dictionary['title']}\")\n",
    "    print(f\"WORDS: {', '.join(sorted(dictionary['words']))}\\n{'-'*20}\")\n",
    "\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d2636-97c5-4c7d-9fe1-2da77aad7400",
   "metadata": {},
   "source": [
    "These words were selected for their relevance in business communications. Yet our texts are movie reviews. We should probably see whether these dictionaries still make sense.\n",
    "\n",
    "Let's conduct a *concordance* analysis or a Key Word In Context (KWIC) analysis to see if these words have face validity: (Remember that stop words have been removed, so the english won't flow perfectly here...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b04452-037d-426e-95ba-19aeaef34889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks for instances of the 'look_for' word in the corpus and prints their contexts.\n",
    "look_for = \"beat\"\n",
    "nltk.Text([word for id, row in test_data.iterrows() for word in row['review_tokens']]).concordance(look_for)# Counts the number of instances of 'wordlist' words in 'tokens'\n",
    "\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc319c3-b54d-497d-829d-96af47ed0f85",
   "metadata": {},
   "source": [
    "Chances are good that you found some instances that didn't seem quite right.\n",
    "\n",
    "**Why this might be bad:** Language could differ in the current texts than in the texts from which these dictionaries were developed.\n",
    "\n",
    "**Why this might be OK:** Finding some false-positives is inevitable even with the most appropriate dictionaries. We try to minimize this, but it's a balancing act: the false negatives created by omitting a word that is usually used in-context creates measurement error variance as well.\n",
    "\n",
    "Let's hold our nose for a moment and assume we're OK with the current dictionaries. Let's conduct the actual dictionary-based computer-aided text analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cdc9c1-485e-42d6-9e1b-f2dcf0669378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducts and displays Dictionary-based CATA for Henry (2008) positivity and negativity dictionaries\n",
    "test_data[\"positivity_henry_08\"] = test_data[\"review_tokens\"].apply(lambda x: get_count(x, dictionaries['Tone_Positivity_Henry08']['words']))\n",
    "test_data[\"negativity_henry_08\"] = test_data[\"review_tokens\"].apply(lambda x: get_count(x, dictionaries['Tone_Negativity_Henry08']['words']))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc57c5b-15cc-4f92-b003-08e65608c1cd",
   "metadata": {},
   "source": [
    "Now that we have positivity and negativity scores, we can calculate a point-biserial correlation with the actual sentiment to see how accurate we are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96885d-d7c8-43be-aeab-b315a3351a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and displays point-biserial correlations for positive and negative CATA with ground-truth sentiment\n",
    "pos_pbsr = pointbiserialr(x=test_data['sentiment'], y=test_data['positivity_henry_08'])\n",
    "neg_pbsr = pointbiserialr(x=test_data['sentiment'], y=test_data['negativity_henry_08'])\n",
    "\n",
    "print(f\"The correlation between sentiment and the positivity dictionary is {pos_pbsr[0]:.02}; p = {pos_pbsr[1]:.03}\")\n",
    "print(f\"The correlation between sentiment and the negativity dictionary is {neg_pbsr[0]:.02}; p = {neg_pbsr[1]:.03}\")\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c94d6-ab38-4046-a2de-860174885dd7",
   "metadata": {},
   "source": [
    "However, here we have two measures of sentiment rather than one. There are many ways the literature has combined such measures into a single sentiment score. A couple common approaches are:\n",
    "* Difference scores\n",
    "* Janis-Fadner coefficient of imbalance\n",
    "\n",
    "Let's look at how these overall sentiment scores correlate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb27dd-df0b-4b50-b759-751855f20a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the Janis-Fadner Coefficient of Imbalance as a function of positive, negative, and total words\n",
    "def coeff_of_imbalance(pos, neg, tot_words):\n",
    "  if pos == neg:\n",
    "    return 0\n",
    "  elif pos > neg:\n",
    "    return ((pos**2 - pos*neg))/((pos+neg)*tot_words)\n",
    "  elif pos < neg:\n",
    "    return ((neg**2 - pos*neg))/((pos+neg)*tot_words)\n",
    "\n",
    "\n",
    "# Adds sentiment columns for both difference scores and J-F coefficients\n",
    "test_data[\"sentiment_henry_08\"] = test_data[\"positivity_henry_08\"] - test_data[\"negativity_henry_08\"]\n",
    "test_data[\"coeff_imb_henry_08\"] = test_data.apply(lambda row: coeff_of_imbalance(row.positivity_henry_08, row.negativity_henry_08, len(row.review_tokens)), axis=1)\n",
    "\n",
    "\n",
    "# Calculates and displays point-biserial correlations for difference scores and J-F coefficients with ground-truth sentiment\n",
    "sent_pbsr = pointbiserialr(x=test_data['sentiment'], y=test_data['sentiment_henry_08'])\n",
    "coi_pbsr = pointbiserialr(x=test_data['sentiment'], y=test_data['coeff_imb_henry_08'])\n",
    "\n",
    "print(f\"The correlation between sentiment and the difference score is          {sent_pbsr[0]:.02}; p = {sent_pbsr[1]:.03}\")\n",
    "print(f\"The correlation between sentiment and the coefficient of imbalance is  {coi_pbsr[0]:.02}; p = {coi_pbsr[1]:.03}\")\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2532d72-a398-452f-a522-3fa97ed99562",
   "metadata": {},
   "source": [
    "Chances are these word lists need refining... let's use the `scattertext` package to examine the distribution of words over positive/negative sentiment texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d39fa4-06cf-421f-9488-7f574a1cf9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display a scattertext explorer plot\n",
    "st_corpus = st.CorpusFromPandas(test_data, category_col='txt_sent', text_col='review').build().compact(st.AssociationCompactor(2000))\n",
    "st_html = st.produce_scattertext_explorer(st_corpus,\n",
    "                                          category='Positive',\n",
    "                                          category_name='Positive',\n",
    "                                          not_categories=['Negative'],\n",
    "                                          sort_by_dist=False,\n",
    "                                          term_scorer=st.CredTFIDF(st_corpus),\n",
    "                                          background_color='#e5e5e3'\n",
    "                                          )\n",
    "display(HTML(st_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d94fc77-f2a3-44b9-bce6-25940b7bfad1",
   "metadata": {},
   "source": [
    "We can use this chart to see words that are:\n",
    "\n",
    "*   Used frequently in negative reviews and infrequently in positive reviews, but are not listed in our negative word list. (False Negatives)\n",
    "*   Used frequently in positive reviews and infrequently in negative reviews, but are not listed in our positive word list. (False Negatives)\n",
    "*   in our word lists, but do not discriminate well between positive and negative reviews. (False Positives)\n",
    "\n",
    "As a reminder, here is what the Henry (2008) word lists contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e912b-0751-4f80-a6ee-b4ce7a04cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out all dictionaries' names, titles, and word lists.\n",
    "for name, dictionary in dictionaries.items():\n",
    "    print(f\"\\nNAME: {name}\")\n",
    "    print(f\"TITLE: {dictionary['title']}\")\n",
    "    print(f\"WORDS: {', '.join(sorted(dictionary['words']))}\\n{'-'*20}\")\n",
    "\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99541cd0-d351-4be8-94d3-cf9e94a8df4c",
   "metadata": {},
   "source": [
    "#### ACTIVITY\n",
    "---\n",
    "\n",
    "* Use the scattertext plot to add words to the positive/negative dictionaries based on:\n",
    "  * How well they discriminate between positive and negative reviews\n",
    "  * Whether you could theoretically justify their linkage to positive/negative sentiment (e.g., Just because 'Seagal' tends to be in bad movies, doesn't mean we should use his name to influence negative sentiment scores.\n",
    "* Go through the existing dictionaries and remove words that seem to cause problems in the dictionary-based analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2286d-2dd1-4db7-a821-7169f3599656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~CHANGE THESE LISTS~~~~~\n",
    "add_to_positive = [\"great\", \"brilliant\", \"perfect\", \"wonderful\", \"favorite\", \"loved\"]\n",
    "add_to_negative = [\"worst\", \"terrible\", \"awful\", \"waste\"]\n",
    "remove_from_positive = [\"\"]\n",
    "remove_from_negative = [\"declined\"]\n",
    "\n",
    "\n",
    "# Runs the CATA for the custom dictionaries and calculates overall sentiment scores\n",
    "custom_pos = list(set(dictionaries['Tone_Positivity_Henry08']['words']+add_to_positive)-set(remove_from_positive))\n",
    "custom_neg = list(set(dictionaries['Tone_Negativity_Henry08']['words']+add_to_negative)-set(remove_from_negative))\n",
    "test_data[\"positivity_custom\"] = test_data[\"review_tokens\"].apply(lambda x: get_count(x, custom_pos))\n",
    "test_data[\"negativity_custom\"] = test_data[\"review_tokens\"].apply(lambda x: get_count(x, custom_neg))\n",
    "test_data[\"sentiment_custom\"] = test_data[\"positivity_custom\"] - test_data[\"negativity_custom\"]\n",
    "test_data[\"coeff_imb_custom\"] = test_data.apply(lambda row: coeff_of_imbalance(row.positivity_custom, row.negativity_custom, len(row.review_tokens)), axis=1)\n",
    "\n",
    "\n",
    "# Calculates point-biserial correlations for custom dictionaries with ground-truth sentiment\n",
    "cus_pos_pbsr = pointbiserialr(x=test_data['sentiment'], y=test_data['positivity_custom'])\n",
    "cus_neg_pbsr = pointbiserialr(x=test_data['sentiment'], y=test_data['negativity_custom'])\n",
    "cus_sent_pbsr = pointbiserialr(x=test_data['sentiment'], y=test_data['sentiment_custom'])\n",
    "cus_coi_pbsr = pointbiserialr(x=test_data['sentiment'], y=test_data['coeff_imb_custom'])\n",
    "\n",
    "\n",
    "# Displays comparison of Henry (08) and custom dictionaries based on point-biserial corellations\n",
    "print(f\"The correlation between sentiment and the positivity dictionary is......... ORIGINAL: {pos_pbsr[0]:.02}; p = {pos_pbsr[1]:.02} --- CUSTOM: {cus_pos_pbsr[0]:.02}; p = {cus_pos_pbsr[1]:.02}\")\n",
    "print(f\"The correlation between sentiment and the negativity dictionary is......... ORIGINAL: {neg_pbsr[0]:.02}; p = {neg_pbsr[1]:.02} --- CUSTOM:{cus_neg_pbsr[0]:.02}; p = {cus_neg_pbsr[1]:.02}\")\n",
    "print(f\"The correlation between sentiment and the difference score is.............. ORIGINAL: {sent_pbsr[0]:.02}; p = {sent_pbsr[1]:.02} --- CUSTOM: {cus_sent_pbsr[0]:.02}; p = {cus_sent_pbsr[1]:.02}\")\n",
    "print(f\"The correlation between sentiment and the coefficient of imbalance is...... ORIGINAL: {coi_pbsr[0]:.02}; p = {coi_pbsr[1]:.02} --- CUSTOM: {cus_coi_pbsr[0]:.02}; p = {cus_coi_pbsr[1]:.02}\")\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c7c5d4-d4d1-44a9-a04e-cb21a84e0c58",
   "metadata": {},
   "source": [
    "## VADER (and other weighted rules-based sentiment analyses)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b280b-0591-49e6-a40d-e8192f3472ca",
   "metadata": {},
   "source": [
    "VADER is an acronym for \"**V**alence **A**ware **D**ictionary and s**E**ntiment **R**easoner\" and builds on basic dictionary-based computer-aided text anaylses by [applying weights to sentiment words](https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt).\n",
    "\n",
    "For example, the word '*awesome*' has a score of 3.1, whereas '*nice*' has a score of 1.8, and '*horrible*' has a score of -2.5.\n",
    "\n",
    "The VADER algorithm also explicitly addresses negation (e.g., 'isn't horrible') and boosting (e.g., 'very horrible') in a way that routine dictionary-based approaches do not.\n",
    "\n",
    "Let's take a look at some examples of what VADER produces for some sample phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a161519-ee1e-47f3-8c58-84fc8fbee93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have VADER code four sample sentences and display the results\n",
    "vader_coder = SentimentIntensityAnalyzer()\n",
    "good_phrase = \"This is an awesome movie\"\n",
    "bad_phrase = \"This is a horrible movie\"\n",
    "negated_bad = \"This isn't a horrible movie\"\n",
    "boosted_bad = \"This is a very horrible movie\"\n",
    "\n",
    "print(f\"'{good_phrase}' is coded as {vader_coder.polarity_scores(good_phrase)}\")\n",
    "print(f\"'{bad_phrase}' is coded as {vader_coder.polarity_scores(bad_phrase)}\")\n",
    "print(f\"'{negated_bad}' is coded as {vader_coder.polarity_scores(negated_bad)}\")\n",
    "print(f\"'{boosted_bad}' is coded as {vader_coder.polarity_scores(boosted_bad)}\")\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b626ac8-4862-49cf-a274-6907866de861",
   "metadata": {},
   "source": [
    "On face, this seems a significant improvement over what we would see with a generic dictionary-based computer-aided text analysis.\n",
    "\n",
    "Let's see how well it compares quantitatively on our corpus of IMDB movie reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ccbb59-844f-4cc0-86a4-3d84fbcecd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a VADER sentiment column to the DataFrame based on the compound score for each text\n",
    "test_data[\"vader_comp\"] = test_data[\"review\"].apply(lambda x: vader_coder.polarity_scores(x)['compound'])\n",
    "\n",
    "\n",
    "# Correlate the VADER sentiment with the ground-truth sentiment and display the results (alongside previous results)\n",
    "vader_pbsr = pointbiserialr(x=test_data['sentiment'], y=test_data['vader_comp'])\n",
    "\n",
    "print(f\"The correlation between sentiment and the VADER score is................... ORIGINAL: {vader_pbsr[0]:.02}; p = {vader_pbsr[1]:.02}\")\n",
    "print(f\"{'-'*120}\")\n",
    "print(f\"The correlation between sentiment and the difference score is.............. ORIGINAL: {sent_pbsr[0]:.02}; p = {sent_pbsr[1]:.02} --- CUSTOM: {cus_sent_pbsr[0]:.02}; p = {cus_sent_pbsr[1]:.02}\")\n",
    "print(f\"The correlation between sentiment and the coefficient of imbalance is...... ORIGINAL: {coi_pbsr[0]:.02}; p = {coi_pbsr[1]:.02} --- CUSTOM: {cus_coi_pbsr[0]:.02}; p = {cus_coi_pbsr[1]:.02}\")\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e55d4e7-eabb-44ef-a382-b077f8de2655",
   "metadata": {},
   "source": [
    "Clearly the original Henry (2008) dictionaries didn't fare well against VADER - but that wasn't a fair comparison.\n",
    "\n",
    "In contrast, if you were thorough in your refinement of the positive/negative dictionaries, you may well have matched or even surpassed the correlation from VADER. VADER was developed using social media texts. So while perhaps closer to movie reviews than the business texts used by Henry, even the more nuanced approach to coding used by VADER may struggle to outperform a dictionary custom-developed to your context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc5c8a-b938-4280-9427-5eb7e6acada7",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning\n",
    "---\n",
    "\n",
    "Dictionary-/rules-based approaches are particularly valuable when you do not have a large number of already classified texts. This is often the case in our research, where the reason many business scholars are interested in CATA is because we do not have access to the 'ground truth' for a large number of observations.\n",
    "\n",
    "However, as ground-truth observations become available, we can use sequence classification algorithms in machine learning to have the computer learn to distinguish between positive and negative sentiment in text.\n",
    "\n",
    "However, before we can do that, we must first figuratively teach the computer to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad5f26-2ceb-42c2-81d6-f67f50d2d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts texts into Term Frequency-Inverse Document Frequency (TF-IDF) vectors\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "x_train_tfidf = vectorizer.fit_transform(full_train_data['review'])\n",
    "x_validation_tfidf = vectorizer.transform(full_validation_data['review'])\n",
    "x_test_tfidf = vectorizer.transform(full_test_data['review'])\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40712bda-c30b-4e70-9050-adbaf62fadc5",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b7bbd-2d18-40de-9c43-2c8521bada42",
   "metadata": {},
   "source": [
    "This technique should sound familiar. This workhorse of the statistical world reappears in the machine learning world as a basic model for classification. Here we're regressing the 'ground truth' sentiment on the words used in the text (expressed as a tf-idf vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f96f0-94d4-4090-a9d7-857edc98b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~HYPERPARAMETERS~~~~~\n",
    "penalty = \"l2\"     # penalizes more complex models - options: l2, l1, elasticnet, none\n",
    "C = 1              # Regularization parameter - smaller numbers --> greater regularization\n",
    "solver = 'lbfgs'   # optimization algorithm - lbfgs, newton-cg, liblinear, sag, saga\n",
    "max_iter = 100     # maximum number of iterations - must be an integer\n",
    "\n",
    "\n",
    "# Train the logistic regression classifier\n",
    "lr_classifier = linear_model.LogisticRegression(penalty=penalty, C=C, solver=solver, max_iter=max_iter, n_jobs=-1)\n",
    "lr_classifier.fit(x_train_tfidf,full_train_data['sentiment'])\n",
    "lr_predictions = lr_classifier.predict(x_test_tfidf)\n",
    "\n",
    "\n",
    "# Estimate and print the accuracy and phi coefficient for the logistic regression classifier\n",
    "lr_accuracy = accuracy_score(lr_predictions, full_test_data['sentiment'])\n",
    "lr_phi = matthews_corrcoef(lr_predictions, full_test_data['sentiment'])\n",
    "\n",
    "print(f\"Logistic Regression accuracy: {lr_accuracy:.2%}\")\n",
    "print(f\"Logistic Regression phi coefficient (correlation): {lr_phi:.02}\")\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901dfffe-25e6-402e-988e-4d26a2f26b10",
   "metadata": {},
   "source": [
    "### Naïve Bayes\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d2caa-ae70-468e-ab30-ddc21df8227e",
   "metadata": {},
   "source": [
    "Like with logistic regression, you have likely worked with a foundational component of the naïve Bayes classifier in statistics. This classifier uses a key insight from Bayesian statistics to use words to predict a classification. Specifically, we're going to use Bayes' rule to find the P(classification|words) given the P(words|classification), P(classification), and P(words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106313e4-94c1-4d22-bcb1-dc1e4dab1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~HYPERPARAMETERS~~~~~\n",
    "alpha = 1.00       # Additive smoothing parameter - Float\n",
    "fit_prior = True   # Whether to learn the prior class probabilities from data or use uniform prior\n",
    "\n",
    "\n",
    "# Train the naive bayes classifier\n",
    "nb_classifier = naive_bayes.MultinomialNB(alpha=alpha, fit_prior=fit_prior)\n",
    "nb_classifier.fit(x_train_tfidf,full_train_data['sentiment'])\n",
    "nb_predictions = nb_classifier.predict(x_test_tfidf)\n",
    "\n",
    "\n",
    "# Estimate and print the accuracy and phi coefficient for the naive bayes classifier\n",
    "nb_accuracy = accuracy_score(nb_predictions, full_test_data['sentiment'])\n",
    "nb_phi = matthews_corrcoef(nb_predictions, full_test_data['sentiment'])\n",
    "\n",
    "print(f\"Naive Bayes' accuracy: {nb_accuracy:.2%}\")\n",
    "print(f\"Naive Bayes' phi coefficient (correlation): {nb_phi:.02}\")\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2219e77-882a-4650-8f26-1a938971a4f3",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433ecf8-954a-4819-b831-7b167b55a768",
   "metadata": {},
   "source": [
    "The random forest classifier is what is called an 'ensemble' classifier. That is, the random forest classifier actually solicits the classifications from a number of other classifiers and treats them as 'votes', tallies the votes and produces a classification on that basis. It's a little more complicated than that, but this is the basic idea. The reason it is called a 'random **forest**'? Well, the classifiers is solicits votes from are 'decision **trees**.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20226dd9-a7b1-4124-bb77-23825ab0cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~HYPERPARAMETERS~~~~~\n",
    "n_estimators = 100      # Number of trees in the forest\n",
    "criterion = 'gini'      # Measure of the quality of a split in the decision tree - gini or entropy\n",
    "max_depth = None        # Maximum depth the trees in the forest can have - None or integer\n",
    "min_samples_split = 2   # Minimum number of texts in the branch when a split is made - integer (technically you can have a float, but stick with integer)\n",
    "min_samples_leaf = 1    # Minimum number of texts in a leaf on the decision tree - integer (technically you can have a float, but stick with integer)\n",
    "\n",
    "\n",
    "# Train the random forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth,\n",
    "                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, n_jobs=-1)\n",
    "rf_classifier.fit(x_train_tfidf,full_train_data['sentiment'])\n",
    "rf_predictions = rf_classifier.predict(x_test_tfidf)\n",
    "\n",
    "\n",
    "# Estimate and print the accuracy and phi coefficient for the random forest classifier\n",
    "rf_accuracy = accuracy_score(rf_predictions, full_test_data['sentiment'])\n",
    "rf_phi = matthews_corrcoef(rf_predictions, full_test_data['sentiment'])\n",
    "\n",
    "print(f\"Random Forest accuracy: {rf_accuracy:.2%}\")\n",
    "print(f\"Random Forest phi coefficient (correlation): {rf_phi:.02}\")\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd7f657-4dde-4f98-8988-c9d6723fedf2",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4133505-a46a-4566-b3b1-c3f303e67169",
   "metadata": {},
   "source": [
    "The support vector machine has become a very popular classifier for its flexibility. It handles high feature/sample size ratios well, supports several kernel functions for when the decision boundary between two sets are not linearly separable, and is pretty memory efficient for what it does.\n",
    "\n",
    "It can, however, quite slow... be prepared to wait on this one for a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd78af-067a-47c1-9a96-70cb740df4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~HYPERPARAMETERS~~~~~\n",
    "C = 1.0             # Regularization parameter - smaller numbers --> greater regularization\n",
    "kernel = 'linear'   # Kernel type to use - linear, poly, rbf, sigmoid\n",
    "degree = 3          # Degree of polynomial kernel (used if you use 'poly' kernel only)\n",
    "gamma = 'auto'      # Kernel coefficient for rbf, poly, and sigmoid kernels - auto, scale or a floating point number\n",
    "\n",
    "\n",
    "# Train the support vector machine classifier\n",
    "svm_classifier = svm.SVC(C=C, kernel=kernel, degree=degree, gamma=gamma)\n",
    "svm_classifier.fit(x_train_tfidf,full_train_data['sentiment'])\n",
    "svm_predictions = svm_classifier.predict(x_test_tfidf)\n",
    "\n",
    "\n",
    "# Estimate and print the accuracy and phi coefficient for the random forest classifier\n",
    "svm_accuracy = accuracy_score(svm_predictions, full_test_data['sentiment'])\n",
    "svm_phi = matthews_corrcoef(svm_predictions, full_test_data['sentiment'])\n",
    "\n",
    "print(f\"Support Vector Machine accuracy: {svm_accuracy:.2%}\")\n",
    "print(f\"Support Vector Machine phi coefficient (correlation): {svm_phi:.02}\")\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820c8a7-f176-4d33-9c91-02cc511a9820",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518eac8-6eef-4ff8-83ed-e953701d247a",
   "metadata": {},
   "source": [
    "Gradient boosting is an ensemble technique that combines multiple weak learners (e.g., decision trees) similar to the random forest approach we explored above. However, unlike a random forest which looks at individual trees in parallel, gradient boosting constructs trees sequentially, where each new tree aims to address the errors of the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a55688-454a-4d1b-a1d8-aad961d0eb9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ~~~~~HYPERPARAMETERS~~~~~\n",
    "n_estimators = 1000 # Number of 'boosting' rounds\n",
    "max_depth = 2 # Maximum tree depth\n",
    "max_leaves = 0 # Maximum number of leaves (set to zero for no limit)\n",
    "learning_rate = .01 # Boosting learning rate (eta)\n",
    "objective = \"binary:logistic\"\n",
    "\n",
    "# Train the gradient boosting classifier\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, objective=objective, n_jobs=-1)\n",
    "xgb_classifier.fit(x_train_tfidf,full_train_data['sentiment'], eval_set=[(x_validation_tfidf, full_validation_data['sentiment'])], verbose=0)\n",
    "xgb_predictions = xgb_classifier.predict(x_test_tfidf)\n",
    "\n",
    "\n",
    "# Estimate and print the accuracy and phi coefficient for the gradient boosting classifier\n",
    "xgb_accuracy = accuracy_score(xgb_predictions, full_test_data['sentiment'])\n",
    "xgb_phi = matthews_corrcoef(xgb_predictions, full_test_data['sentiment'])\n",
    "\n",
    "print(f\"Gradient Boosting accuracy: {xgb_accuracy:.2%}\")\n",
    "print(f\"Gradient Boosting phi coefficient (correlation): {xgb_phi:.02}\")\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7486d-4825-48e4-bd9e-fd5758fa64a4",
   "metadata": {},
   "source": [
    "### ACTIVITY\n",
    "---\n",
    "\n",
    "In the previous sections we used more-or-less out-of-the-box numbers for the hyperparameters of the machine learning classifiers. Much like other applications of machine learning, you can generally obtain better sentiment classification accuracy by toying around with the parameters a bit.\n",
    "\n",
    "In the sections above, tweak the **hyperparameter** values and see if you can find models that outperform the base model provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc11135e-6ec4-4f1c-ab1a-1ec70c75ab5b",
   "metadata": {},
   "source": [
    "### Hyperparameter search and tuning\n",
    "---\n",
    "\n",
    "In the above activity, you manually tweaked the machine learning algorithms' hyperparameters to find the best performing model. In practice, however, we generally let the computer do some of this exploration. There are many approaches for doing so, one of these is to do a 'grid search' where you provide the parameters to test and the computer will examine all permutations of these.\n",
    "\n",
    "You will likely see Convergence warnings, ignore them for the time being - those are models that did not converge in under 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8311e5-34ac-479c-9e70-b0e54ba9c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~HYPERPARAMETERS~~~~~\n",
    "params = {\n",
    "    'penalty': [\"l2\", \"l1\"],   \n",
    "    'C': np.logspace(-3,3),\n",
    "    'solver': [\"lbfgs\", \"liblinear\"]\n",
    "}\n",
    "\n",
    "# Train the logistic regression classifier with grid search\n",
    "grid_classifier = linear_model.LogisticRegression(max_iter=1000)\n",
    "grid = GridSearchCV(estimator=grid_classifier, param_grid=params, scoring='roc_auc', n_jobs=-1, verbose=0)\n",
    "grid.fit(x_train_tfidf,full_train_data['sentiment'])\n",
    "\n",
    "print(f\"\\nBest estimator: {grid.best_estimator_}\")\n",
    "grid_predictions = grid.best_estimator_.predict(x_test_tfidf)\n",
    "\n",
    "# Estimate and print the accuracy and phi coefficient for the best LR classifier from the grid search\n",
    "grid_accuracy = accuracy_score(grid_predictions, full_test_data['sentiment'])\n",
    "grid_phi = matthews_corrcoef(grid_predictions, full_test_data['sentiment'])\n",
    "\n",
    "print(f\"\\nGrid Search LR accuracy: {grid_accuracy:.2%}\")\n",
    "print(f\"Grid Search LR phi coefficient (correlation): {grid_phi:.02}\")\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53519009-acbe-4510-9142-6353acbb25aa",
   "metadata": {},
   "source": [
    "Another consideration when identifying hyperparameters is cross-validation. In k-fold cross-validation, the data is divided into *k* subsamples or \"folds\". The model training and validation process is then repeated k times, with each of the k folds serving as the validation dataset once, and the remaining k-1 folds comprise the training set. This approach ensures that every data point gets to be in the validation set exactly once and in the training set k-1 times, providing a thorough way of assessing the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d492192-d1a8-4ec8-bd51-30bc6d27cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~HYPERPARAMETERS~~~~~\n",
    "params = {\n",
    "    'penalty': [\"l2\", \"l1\"],   \n",
    "    'C': np.logspace(-3,3),\n",
    "    'solver': [\"lbfgs\", \"liblinear\"]\n",
    "}\n",
    "k_folds = 5\n",
    "\n",
    "# Train the logistic regression classifier\n",
    "kgrid_classifier = linear_model.LogisticRegression(max_iter=1000)\n",
    "kgrid = GridSearchCV(estimator=kgrid_classifier, param_grid=params, scoring='roc_auc', cv=k_folds, n_jobs=-1, verbose=0)\n",
    "kgrid.fit(x_train_tfidf,full_train_data['sentiment'])\n",
    "print(f\"\\nBest estimator: {kgrid.best_estimator_}\")\n",
    "\n",
    "kgrid_predictions = kgrid.best_estimator_.predict(x_test_tfidf)\n",
    "\n",
    "# Estimate and print the accuracy and phi coefficient for the best LR classifier\n",
    "kgrid_accuracy = accuracy_score(kgrid_predictions, full_test_data['sentiment'])\n",
    "kgrid_phi = matthews_corrcoef(kgrid_predictions, full_test_data['sentiment'])\n",
    "\n",
    "print(f\"\\nGrid Search LR accuracy: {kgrid_accuracy:.2%}\")\n",
    "print(f\"Grid Search LR phi coefficient (correlation): {kgrid_phi:.02}\")\n",
    "print(\"\\nReady for next step...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719fa31-2919-4df9-b8a4-53e3b78bcf0f",
   "metadata": {},
   "source": [
    "### A Final Comparison\n",
    "---\n",
    "\n",
    "This wraps up the comparison of the sentiment analysis machine learning classification algorithms. As I hope you've seen, this is not a one-size-fits-all decision, there are a number of factors that should enter into your calculus for not only deciding what tool to use, but how to tune and use it.\n",
    "\n",
    "That being said, the below code will consolidate all of the statistics for the models we ran (net of any tinkering you may have done as part of the activities, of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc47208-b2c5-4810-8074-c95332396441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of each sentiment approach\n",
    "print(\"CORRELATIONS:\")\n",
    "print(f\"The correlation between sentiment and the positivity dictionary is......... ORIGINAL: {pos_pbsr[0]:.02}; p = {pos_pbsr[1]:.02e} --- CUSTOM: {cus_pos_pbsr[0]:.02}; p = {cus_pos_pbsr[1]:.02e}\")\n",
    "print(f\"The correlation between sentiment and the negativity dictionary is......... ORIGINAL: {neg_pbsr[0]:.02}; p = {neg_pbsr[1]:.02e} --- CUSTOM:{cus_neg_pbsr[0]:.02}; p = {cus_neg_pbsr[1]:.02e}\")\n",
    "print(f\"The correlation between sentiment and the difference score is.............. ORIGINAL: {sent_pbsr[0]:.02}; p = {sent_pbsr[1]:.02e} --- CUSTOM: {cus_sent_pbsr[0]:.02}; p = {cus_sent_pbsr[1]:.02e}\")\n",
    "print(f\"The correlation between sentiment and the coefficient of imbalance is...... ORIGINAL: {coi_pbsr[0]:.02}; p = {coi_pbsr[1]:.02e} --- CUSTOM: {cus_coi_pbsr[0]:.02}; p = {cus_coi_pbsr[1]:.02e}\")\n",
    "print(f\"The Logistic Regression phi coefficient (correlation) with sentiment is.............. {lr_phi:.02}\")\n",
    "print(f\"The Naive Bayes' phi coefficient (correlation) with sentiment is..................... {nb_phi:.02}\")\n",
    "print(f\"The Random Forest phi coefficient (correlation) with sentiment is.................... {rf_phi:.02}\")\n",
    "print(f\"The Support Vector Machine phi coefficient (correlation) with sentiment is........... {svm_phi:.02}\")\n",
    "print(f\"The Gradient Boosting phi coefficient (correlation) with sentiment is................ {xgb_phi:.02}\")\n",
    "print(f\"The Grid Search Logistic Regression phi coefficient (correlation) with sentiment is.. {grid_phi:.02}\")\n",
    "print(f\"The K-Fold CV Logistic Regression phi coefficient (correlation) with sentiment is.... {kgrid_phi:.02}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{'-'*120}\")\n",
    "print(\"ACCURACIES:\")\n",
    "print(f\"Logistic Regression accuracy................... {lr_accuracy:.2%}\")\n",
    "print(f\"Naive Bayes' accuracy.......................... {nb_accuracy:.2%}\")\n",
    "print(f\"Random Forest accuracy......................... {rf_accuracy:.2%}\")\n",
    "print(f\"Support Vector Machine accuracy................ {svm_accuracy:.2%}\")\n",
    "print(f\"The Gradient Boosting accuracy................. {xgb_accuracy:.2%}\")\n",
    "print(f\"The Grid Search Logistic Regression accuracy... {grid_accuracy:.2%}\")\n",
    "print(f\"The K-Fold CV Logistic Regression accuracy..... {kgrid_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3bb05f-c7e0-406b-bd41-8436d9ae5c7d",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning\n",
    "---\n",
    "\n",
    "With supervised machine learning we provided the algorithm with both the inputs and desired output. Specifically, we used a 'classification' algorithm to use text inputs to predict a categorical output (i.e., sentiment). In unsupervised machine learning algorithms, we provide the algorithm with the inputs, but we do not provide it with a desired output.\n",
    "\n",
    "To provide an analogy to statistical techniques we use in academia:\n",
    "* Supervised machine learning is like OLS, logistic regression, and SEM - We provide both X and Y variables and the algorithm figures out the best way to link them based on predetermined rules\n",
    "* Unsupervised machine learning is like exploratory factor analysis, principal components analysis, and cluster analysis - We provide only X variables and the algorithm determines how to combine those X variables in ways that help us understand more about those variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef03e6-d850-4259-9fdd-8abb1e98ed7c",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "There are many unsupervised machine learning algorithms, but a popular one in management research right now is topic modeling.\n",
    "\n",
    "Topic models are designed to discover the \"topics\" that occur in a corpus of documents. These models assume that documents are mixtures of topics, which themselves are characterized as a distribution over words. To provide a bit of an oversimplification, but one which academics might find accessible, topic models are a bit like a factor analysis of words. Words that 'hang together' frequently become associated with a latent \"topic\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4cc7a-5cfe-4015-8b6e-88003477dc68",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "---\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a foundational topic modeling technique used in Natural Language Processing. When applying LDA, a researcher specifies the number of topics to be extracted from the corpus *a priori* (k), and the algorithm then iteratively learns the word distributions for each topic and the topic distributions for each document. \n",
    "\n",
    "The below code uses LDA to uncover topics associated with the IMDB dataset we have been working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d03342f-20cb-4f5c-b06c-7a4aff638cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~HYPERPARAMETERS~~~~~\n",
    "k = 10 # the number of topics\n",
    "term_weight = tp.TermWeight.ONE # Term weighting scheme: tp.TermWeight.ONE | tp.TermWeight.PMI | tp.TermWeight.IDF\n",
    "min_cf = 3 # Minimum corpus frequency (throw out words that do not appear at least this many times in the corpus)\n",
    "min_df = 1 # Minimum document frequency (throw out words that do not appear at least this many times in a document)\n",
    "rm_top = 5 # Remove the top N words in terms of frequency\n",
    "alpha = .1 # hyperparameter of Dirichlet distribution for document-topic\n",
    "eta = .01 # hyperparameter of Dirichlet distribution for topic-word\n",
    "\n",
    "# Train Hierarchical Dirichlet Process\n",
    "model = tp.LDAModel(tw=term_weight, min_cf=min_cf, min_df=min_df, rm_top=rm_top, alpha=alpha, eta=eta, k=k)\n",
    "for review_text in test_data['review_tokens']:\n",
    "    model.add_doc(review_text)\n",
    "model.burn_in=100\n",
    "model.train(0)\n",
    "print('Number of documents:', len(model.docs), ', Vocab size:', len(model.used_vocabs), ', Number of words:', model.num_words)\n",
    "print('Removed top words:', model.removed_top_words)\n",
    "print('Training...', file=sys.stderr, flush=True)\n",
    "model.train(1000)\n",
    "\n",
    "topic_term_dists = np.stack([model.get_topic_word_dist(k) for k in range(model.k)])\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in model.docs])\n",
    "doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
    "doc_lengths = np.array([len(doc.words) for doc in model.docs])\n",
    "vocab = list(model.used_vocabs)\n",
    "term_frequency = model.used_vocab_freq\n",
    "\n",
    "for k in range(model.k):\n",
    "    print(f\"Topic #{k}\")\n",
    "    for word, prob in model.get_topic_words(k):\n",
    "        print('\\t', word, prob, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb87fb3-3c6f-40f5-8f77-95967bf4931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists, \n",
    "    doc_topic_dists, \n",
    "    doc_lengths, \n",
    "    vocab, \n",
    "    term_frequency,\n",
    "    start_index=0,\n",
    "    sort_topics=False\n",
    ")\n",
    "pyLDAvis.display(prepared_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f1bacc-5a7f-4196-b648-3e7a6649bf86",
   "metadata": {},
   "source": [
    "### Hierarchical Dirichlet Process\n",
    "---\n",
    "\n",
    "With Latent Dirichlet Allocation, the researcher specifies *a priori* the number of topics to be found in the corpus of documents. Unfortunately, there is often no *a priori* theory regarding this number. And if there is such theory, there's no way to ensure that those topics are the ones that will emerge in the data. An alternative algorithm that lets the data determine the number of topics is the Hierarchical Dirichlet Process (HDP) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8108e0-2a3b-4b2e-b40a-63f4012491e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~HYPERPARAMETERS~~~~~\n",
    "term_weight = tp.TermWeight.IDF # Term weighting scheme: tp.TermWeight.ONE | tp.TermWeight.PMI | tp.TermWeight.IDF\n",
    "min_cf = 3 # Minimum corpus frequency (throw out words that do not appear at least this many times in the corpus)\n",
    "min_df = 1 # Minimum document frequency (throw out words that do not appear at least this many times in a document)\n",
    "rm_top = 5 # Remove the top N words in terms of frequency\n",
    "alpha = 0.1 # concentration coefficient of Dirichlet Process for document-table\n",
    "eta = 0.01 # hyperparameter of Dirichlet distribution for topic-word\n",
    "gamma = 0.1 # concentration coeficient of Dirichlet Process for table-topic\n",
    "initial_k = 10\n",
    "\n",
    "# Train Hierarchical Dirichlet Process\n",
    "model = tp.HDPModel(tw=term_weight, min_cf=min_cf, min_df=min_df, rm_top=rm_top, alpha=alpha, eta=eta, gamma=gamma, initial_k=initial_k)\n",
    "for review_text in test_data['review_tokens']:\n",
    "    model.add_doc(review_text)\n",
    "model.burn_in=100\n",
    "model.train(0)\n",
    "print('Number of documents:', len(model.docs), ', Vocab size:', len(model.used_vocabs), ', Number of words:', model.num_words)\n",
    "print('Removed top words:', model.removed_top_words)\n",
    "print('Training...', flush=True)\n",
    "model.train(1000)\n",
    "\n",
    "live_topics = [k for k in range(model.k) if model.is_live_topic(k)]\n",
    "\n",
    "topic_term_dists = np.stack([model.get_topic_word_dist(k) for k in range(model.k)])\n",
    "topic_term_dists = topic_term_dists[live_topics]\n",
    "topic_term_dists /= topic_term_dists.sum(axis=1, keepdims=True)\n",
    "\n",
    "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in model.docs])\n",
    "doc_topic_dists = doc_topic_dists[:, live_topics]\n",
    "doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
    "\n",
    "doc_lengths = np.array([len(doc.words) for doc in model.docs])\n",
    "vocab = list(model.used_vocabs)\n",
    "term_frequency = model.used_vocab_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bced14-179f-44c3-91a6-fcbe476c40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists, \n",
    "    doc_topic_dists, \n",
    "    doc_lengths, \n",
    "    vocab, \n",
    "    term_frequency,\n",
    "    start_index=0,\n",
    "    sort_topics=False\n",
    ")\n",
    "pyLDAvis.display(prepared_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba621c0-7037-4cf0-acdb-de43bf90b703",
   "metadata": {},
   "source": [
    "### ACTIVITY\n",
    "---\n",
    "\n",
    "In the previous section, I provided out-of-the-box numbers for the hyperparameters of the topic models. Tweaking these hyperparameters can influence the interpretability of the topics that emerge. In the sections above, tweak the **hyperparameter** values and see if you can find models that produce interesting/insightful topics regarding the movie reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
